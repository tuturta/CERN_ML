{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lWwBeoo63Zp",
        "outputId": "e7768fe0-f743-4a75-b5c5-3abd9a0b08b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' I HAVE TO NORMALIZE THE LABELS TO 1 USING THE MAX AREA Y = PI*10^2'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a GNN dataset with k-NNGraphs with Torch Geometric\n",
        "from dataset import *\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "''' I HAVE TO NORMALIZE THE LABELS TO 1 USING THE MAX AREA Y = PI*10^2'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cjy3UOlVUz8o"
      },
      "outputs": [],
      "source": [
        "# Dataset parameters\n",
        "n_samples  = 50000\n",
        "\n",
        "# Load the full dataset\n",
        "dataset = test(root='Data')\n",
        "\n",
        "# Split into train and validation set\n",
        "dataset = dataset.shuffle()\n",
        "\n",
        "train_size = round(0.8*len(dataset))\n",
        "val_size  = n_samples - train_size\n",
        "\n",
        "train_dataset = dataset[:train_size]\n",
        "val_dataset = dataset[val_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOGTsII_E2wW",
        "outputId": "25d6e4b1-91cd-4d2a-b3c0-ed9ed4b76de2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset: test(50000):\n",
            "====================\n",
            "Number of graphs: 50000\n",
            "Number of features: 2\n",
            "Number of classes: 49972\n",
            "\n",
            "Data(x=[318, 2], edge_index=[2, 3180], y=[1])\n",
            "=============================================================\n",
            "Number of nodes: 318\n",
            "Number of edges: 3180\n",
            "Average node degree: 10.00\n",
            "Has isolated nodes: False\n",
            "Has self-loops: True\n",
            "Is undirected: False\n",
            "tensor([191.8085])\n"
          ]
        }
      ],
      "source": [
        "# Getting some insights about the first graph\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('====================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[0]  # Get the first graph object.\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "print('=============================================================')\n",
        "\n",
        "# Gather some statistics about the first graph.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {data.has_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')\n",
        "print(data.y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j53Cpc4sAple",
        "outputId": "ef8aee8e-028b-4594-a872-4a66b14d9755"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "batch_size= 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle = True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "40000"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "625*64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[9.9500e-04, 9.9833e-05],\n",
              "        [1.1751e-02, 1.1790e-03],\n",
              "        [2.2506e-02, 2.2582e-03],\n",
              "        ...,\n",
              "        [8.9474e+00, 5.2632e+00],\n",
              "        [9.1228e+00, 5.2632e+00],\n",
              "        [9.2982e+00, 5.2632e+00]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_loader.batch_size\n",
        "next(iter(train_loader)).x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "625.0"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataset)/64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecQZqeVQZNKu",
        "outputId": "d0af5d63-31ca-40a5-f50f-c8df013a428b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GCN(\n",
            "  (conv1): GCNConv(2, 64)\n",
            "  (conv2): GCNConv(64, 64)\n",
            "  (conv3): GCNConv(64, 64)\n",
            "  (lin): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin   = Linear(hidden_channels, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        # 1. Obtain node embeddings \n",
        "        x = self.conv1(x.float(), edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x.float(), edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x.float(), edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.1, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def loss(self, pred, score):\n",
        "        # Start with MAE loss function, then switch to MSE when the error falls below delta\n",
        "        return F.huber_loss(pred, score, reduction = 'mean', delta = 0.1) # ATTENTION delta dépend des labels 0.1 --> rmax =1 \n",
        "\n",
        "\n",
        "model = GCN(hidden_channels=64)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EarlyStopping():\n",
        "    \"\"\"\n",
        "    Early stopping to stop the training when the loss does not improve after\n",
        "    certain epochs.\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        \"\"\"\n",
        "        :param patience: how many epochs to wait before stopping when loss is\n",
        "               not improving\n",
        "        :param min_delta: minimum difference between new loss and old loss for\n",
        "               new loss to be considered as an improvement\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss == None:\n",
        "            self.best_loss = val_loss\n",
        "        elif self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            # reset counter if validation loss improves\n",
        "            self.counter = 0\n",
        "        elif self.best_loss - val_loss < self.min_delta:\n",
        "            self.counter += 1\n",
        "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                print('INFO: Early stopping')\n",
        "                self.early_stop = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gyMM4JUf4tg2"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCN(hidden_channels=64).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, eps = 1e-7)\n",
        "reduce_lr = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer, factor=0.5, patience=2, min_lr=1e-7, verbose = True)\n",
        "early_stopping = EarlyStopping(patience=4)\n",
        "criterion = model.loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit(model, train_loader, optimizer, criterion):\n",
        "    print('Training')\n",
        "    model.train()\n",
        "    train_running_loss = 0.0\n",
        "    train_running_correct = 0.\n",
        "    train_running_mae = 0.\n",
        "    train_running_mse = 0.\n",
        "    counter = 0\n",
        "    prog_bar = tqdm(enumerate(train_loader), total = len(train_loader))\n",
        "    for data in prog_bar:  # Iterate in batches over the training dataset.\n",
        "        counter += 1\n",
        "        data = data[1].to(device)\n",
        "        target = data.y.float()/(np.pi*10**2) # Normalize labels to 1\n",
        "        # zero the parameter gradients\n",
        "        model.zero_grad()\n",
        "        preds = model(data)[:,0]  # Perform a single forward pass.\n",
        "        loss = criterion(preds, target)\n",
        "        # update metrics \n",
        "        train_running_loss += loss.item()\n",
        "        train_running_mse += F.mse_loss(preds,target)\n",
        "        train_running_mae += F.l1_loss(preds, target)\n",
        "\n",
        "        loss.backward()\n",
        "        # Update parameters based on gradients.\n",
        "        optimizer.step() \n",
        "\n",
        "    # compute the mean metrics over the all batch\n",
        "    train_loss = train_running_loss / counter\n",
        "    train_mse = train_running_mse/counter\n",
        "    train_mae = train_running_mae/counter\n",
        "\n",
        "    return train_loss, train_mse, train_mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate(model, val_dataloader, criterion):\n",
        "    print('Validating')\n",
        "    model.eval()\n",
        "    val_running_loss = 0.\n",
        "    val_running_correct = 0.\n",
        "    val_running_mae = 0.\n",
        "    val_running_mse = 0.\n",
        "    counter = 0\n",
        "    prog_bar = tqdm(enumerate(val_dataloader), total=len(val_dataloader))\n",
        "    with torch.no_grad():\n",
        "        for i, data in prog_bar:\n",
        "            counter += 1\n",
        "            data = data.to(device)\n",
        "            target = data.y.float()/(np.pi*10**2) # Normalize labels to 1\n",
        "            preds = model(data)[:,0]\n",
        "            loss = criterion(preds, target)\n",
        "            # update metrics\n",
        "            val_running_loss += loss.item()\n",
        "            val_running_mse += F.mse_loss(preds,target)\n",
        "            val_running_mae += F.l1_loss(preds, target)\n",
        "    \n",
        "    # Compute the mean of metrics over the all batch\n",
        "    val_loss = val_running_loss / counter\n",
        "    val_mse = val_running_mse/counter\n",
        "    val_mae = val_running_mae/counter\n",
        "\n",
        "    return val_loss, val_mse, val_mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_checkpoint(epoch, loss, save_path):\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, 'History/' + save_path + '.pt')\n",
        "\n",
        "def load_checkpoint(model, optimizer, load_path):\n",
        "    checkpoint = torch.load(load_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    \n",
        "    return model, optimizer, epoch\n",
        "\n",
        "def save_history(epochs, optimizer_param, loss, timestamp):\n",
        "    torch.save({\n",
        "            'epoch': epochs,\n",
        "            'optimizer_state_dict': optimizer_param,\n",
        "            # we don't save model weights and biases every epochs to save memory \n",
        "            'Loss': loss,\n",
        "            }, 'History/history + ' + timestamp + '.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 of 10\n",
            "Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [05:44<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [02:35<00:00,  4.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0059, Train MAE: 0.09\n",
            "Validation Loss: 0.0045, Validation MAE: 0.08\n",
            "Epoch 2 of 10\n",
            "Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [05:47<00:00,  1.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [02:36<00:00,  4.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0040, Train MAE: 0.07\n",
            "Validation Loss: 0.0034, Validation MAE: 0.06\n",
            "Epoch 3 of 10\n",
            "Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [06:04<00:00,  1.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [02:32<00:00,  4.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0035, Train MAE: 0.06\n",
            "Validation Loss: 0.0031, Validation MAE: 0.06\n",
            "Epoch 4 of 10\n",
            "Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [05:59<00:00,  1.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [02:40<00:00,  3.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0032, Train MAE: 0.06\n",
            "Validation Loss: 0.0029, Validation MAE: 0.05\n",
            "Epoch 5 of 10\n",
            "Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [05:46<00:00,  1.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [02:30<00:00,  4.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Early stopping counter 1 of 20\n",
            "Train Loss: 0.0031, Train MAE: 0.06\n",
            "Validation Loss: 0.0031, Validation MAE: 0.05\n",
            "Epoch 6 of 10\n",
            "Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [05:50<00:00,  1.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [02:30<00:00,  4.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0029, Train MAE: 0.05\n",
            "Validation Loss: 0.0028, Validation MAE: 0.05\n",
            "Epoch 7 of 10\n",
            "Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [05:48<00:00,  1.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [02:31<00:00,  4.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0028, Train MAE: 0.05\n",
            "Validation Loss: 0.0026, Validation MAE: 0.05\n",
            "Epoch 8 of 10\n",
            "Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [05:43<00:00,  1.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [02:29<00:00,  4.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Early stopping counter 1 of 20\n",
            "Train Loss: 0.0027, Train MAE: 0.05\n",
            "Validation Loss: 0.0028, Validation MAE: 0.05\n",
            "Epoch 9 of 10\n",
            "Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [05:44<00:00,  1.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [02:30<00:00,  4.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0027, Train MAE: 0.05\n",
            "Validation Loss: 0.0025, Validation MAE: 0.05\n",
            "Epoch 10 of 10\n",
            "Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [05:44<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [02:29<00:00,  4.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0026, Train MAE: 0.05\n",
            "Validation Loss: 0.0023, Validation MAE: 0.05\n",
            "Training time: 83.695 minutes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "import time\n",
        "start = time.time()\n",
        "fname = 'checkpoint' + time.strftime(\"%Y%m%d-%H%M%S\") \n",
        "writer = SummaryWriter()\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "loss = {'Hubert_train':[], 'Hubert_val': [],\n",
        "        'MAE_train':   [], 'MAE_val':    [],\n",
        "        'MSE_train':   [], 'MSE_val':    []}\n",
        "optimizer_state_dicts = []\n",
        "epochs_nb = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
        "    train_epoch_loss, train_epoch_mse, train_epoch_mae = fit(\n",
        "        model, train_loader, optimizer, criterion\n",
        "    )\n",
        "    val_epoch_loss, val_epoch_mse, val_epoch_mae = validate(\n",
        "        model, val_loader, criterion\n",
        "    )\n",
        "    # Keep track of Hubert loss \n",
        "    writer.add_scalars(main_tag=\"Huber_Loss\", \n",
        "                       tag_scalar_dict={\"train_loss\": train_epoch_loss,\n",
        "                                            \"test_loss\": val_epoch_loss},\n",
        "                       global_step=epoch)\n",
        "    loss['Hubert_train'].append(train_epoch_loss)\n",
        "    loss['Hubert_val'].append(val_epoch_loss)\n",
        "\n",
        "    # Keep track of MSE loss\n",
        "    writer.add_scalars(main_tag=\"MSE\", \n",
        "                       tag_scalar_dict={\"train_MSE\": train_epoch_mse,\n",
        "                                            \"test_MSE\": val_epoch_mse},\n",
        "                      global_step=epoch) \n",
        "    loss['MSE_train'].append(train_epoch_mse)\n",
        "    loss['MSE_val'].append(val_epoch_mse)\n",
        "\n",
        "    # Keep track of MAE Loss \n",
        "    writer.add_scalars(main_tag=\"MAE\", \n",
        "                       tag_scalar_dict={\"train_MAE\": train_epoch_mae,\n",
        "                                            \"test_MAE\": val_epoch_mae},\n",
        "                      global_step=epoch)        \n",
        "    loss['MAE_train'].append(train_epoch_mse)\n",
        "    loss['MAE_val'].append(val_epoch_mse)\n",
        "\n",
        "    # update EarlyStopping class\n",
        "    early_stopping(val_epoch_loss)\n",
        "    if early_stopping.counter == 0:\n",
        "    # save a checkpoint if the validation loss has improved for this epoch\n",
        "        save_checkpoint(epoch, val_epoch_loss, fname)\n",
        "    \n",
        "    # save an history\n",
        "    optimizer_state_dicts.append(optimizer.state_dict)\n",
        "    epochs_nb.append(epoch)\n",
        "    save_history(epochs_nb, optimizer_state_dicts, loss, timestamp)\n",
        "    \n",
        "    print(f\"Train Loss: {train_epoch_loss:.4f}, Train MAE: {train_epoch_mae:.2f}\")\n",
        "    print(f'Validation Loss: {val_epoch_loss:.4f}, Validation MAE: {val_epoch_mae:.2f}')\n",
        "    if early_stopping.early_stop:\n",
        "        break\n",
        "end = time.time()\n",
        "print(f\"Training time: {(end-start)/60:.3f} minutes\")\n",
        "writer.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "p2EcYQoZ5VkU",
        "outputId": "10fbc0e1-658d-48c9-acc5-75ed8670c27e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'from IPython.display import Javascript\\nimport torch.nn.functional as F\\n\\nmodel.train()\\n\\nfor epoch in tqdm(range(train_size)):\\n    i = 0\\n    loss_ = []\\n    for data in train_loader:  # Iterate in batches over the training dataset.\\n        data = data.to(device)\\n        prediction = model(data)  # Perform a single forward pass.\\n        label = data.y.to(device)\\n        loss = model.loss(prediction[:,0], data.y.float())\\n        model.zero_grad()\\n        loss.backward()\\n        optimizer.step()  # Update parameters based on gradients.\\n        i+= 1 \\n        loss_.append(loss.item())\\n        \\n\\nFor 1000 epochs of 1000 samples, MSE, batch_size = 128 >> rmse = 1.7 \\n'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''from IPython.display import Javascript\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in tqdm(range(train_size)):\n",
        "    i = 0\n",
        "    loss_ = []\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "        data = data.to(device)\n",
        "        prediction = model(data)  # Perform a single forward pass.\n",
        "        label = data.y.to(device)\n",
        "        loss = model.loss(prediction[:,0], data.y.float())\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "        i+= 1 \n",
        "        loss_.append(loss.item())\n",
        "        \n",
        "\n",
        "For 1000 epochs of 1000 samples, MSE, batch_size = 128 >> rmse = 1.7 \n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" Avoiding overfitting implementing \\n- change to MAE or Huber #done\\n- add learning rate and epsilon to optimizer # done\\n- test on test set every epoch # done\\n- early stopping : if after 20 epochs, the loss doesn't improve anymore, the training stop #done\\n- variable learning rate : if after 5 epochs, the loss doesn't improve anymore : multiply it by 0.1 --> ReduceLRonPlateau # done\\n- checkpoints (only the best) : save the weights, the bias, epochs, lr, ...  --> in a file #done\\n\\n- normalize to 1 \\n- metrics : loss MAE, loss MSE and loss Huber # done\\n- history : loss of train and test each epochs + metrics --> save  #done\\n\""
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "''' Avoiding overfitting implementing \n",
        "- change to MAE or Huber #done\n",
        "- add learning rate and epsilon to optimizer # done\n",
        "- test on test set every epoch # done\n",
        "- early stopping : if after 20 epochs, the loss doesn't improve anymore, the training stop #done\n",
        "- variable learning rate : if after 5 epochs, the loss doesn't improve anymore : multiply it by 0.1 --> ReduceLRonPlateau # done\n",
        "- checkpoints (only the best) : save the weights, the bias, epochs, lr, ...  --> in a file #done\n",
        "\n",
        "- normalize to 1 \n",
        "- metrics : loss MAE, loss MSE and loss Huber # done\n",
        "- history : loss of train and test each epochs + metrics --> save  #done\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow installation not found - running with reduced feature set.\n",
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.10.1 at http://localhost:6006/ (Press CTRL+C to quit)\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!tensorboard --logdir='runs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'step': tensor(6250.),\n",
              " 'exp_avg': tensor([0.0011]),\n",
              " 'exp_avg_sq': tensor([0.0002])}"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimizer.state_dict()['state'][7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.001"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimizer.state_dict()['param_groups'][0]['lr']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "opt = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    foreach: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    momentum: 0.9\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "opt.step()\n",
        "print(opt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit ('3.10.1')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "63412e708ced85145ee3358b7b7540b5b7bcbe0d3c78f470347fabcaf1a15eaf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
